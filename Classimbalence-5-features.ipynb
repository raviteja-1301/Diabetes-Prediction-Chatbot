{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351fdf55-e364-4417-9fae-c6ef6241ff84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready. CSV_PATH: diabetes_binary_health_indicators_BRFSS2015.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 0: Shared config & imports (run this first)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "CSV_PATH = \"diabetes_binary_health_indicators_BRFSS2015.csv\"\n",
    "OUTPUT_DIR = \"model_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# convenience: display full dataframe columns when needed\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(\"Config ready. CSV_PATH:\", CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f890cb2-893e-4844-8aba-bceec718f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframe shape: (253680, 22)\n",
      "\n",
      "Dtype summary (sample):\n",
      "uint8      21\n",
      "float32     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_binary</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>HvyAlcoholConsump</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes_binary  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0                0       1         1          1  40.0       1       0   \n",
       "1                0       0         0          0  25.0       1       0   \n",
       "2                0       1         1          1  28.0       0       0   \n",
       "3                0       1         0          1  27.0       0       0   \n",
       "4                0       1         1          1  24.0       0       0   \n",
       "\n",
       "   HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  \\\n",
       "0                     0             0       0        1                  0   \n",
       "1                     0             1       0        0                  0   \n",
       "2                     0             0       1        0                  0   \n",
       "3                     0             1       1        1                  0   \n",
       "4                     0             1       1        1                  0   \n",
       "\n",
       "   AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  \\\n",
       "0              1            0        5        18        15         1    0   \n",
       "1              0            1        3         0         0         0    0   \n",
       "2              1            1        5        30        30         1    0   \n",
       "3              1            0        2         0         0         0    0   \n",
       "4              1            0        2         3         0         0    0   \n",
       "\n",
       "   Age  Education  Income  \n",
       "0    9          4       3  \n",
       "1    7          6       1  \n",
       "2    9          4       8  \n",
       "3   11          3       6  \n",
       "4   11          5       4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 1: Load & dtype optimization\n",
    "def load_and_optimize_dtypes(csv_path: str, use_sample=False, sample_frac=0.25):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if use_sample:\n",
    "        df = df.sample(frac=sample_frac, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    # Example dtype downcasts (adjust to match your dataset columns)\n",
    "    binary_cols = [\n",
    "        \"HighBP\", \"HighChol\", \"CholCheck\", \"Smoker\", \"Stroke\", \"HeartDiseaseorAttack\",\n",
    "        \"PhysActivity\", \"Fruits\", \"Veggies\", \"HvyAlcoholConsump\", \"AnyHealthcare\",\n",
    "        \"NoDocbcCost\", \"DiffWalk\", \"Sex\"\n",
    "    ]\n",
    "    for c in binary_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"unsigned\")\n",
    "\n",
    "    if \"BMI\" in df.columns:\n",
    "        df[\"BMI\"] = pd.to_numeric(df[\"BMI\"], downcast=\"float\")\n",
    "    for c in [\"MentHlth\", \"PhysHlth\", \"GenHlth\", \"Age\", \"Education\", \"Income\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"unsigned\")\n",
    "\n",
    "    if \"Diabetes_binary\" in df.columns:\n",
    "        df[\"Diabetes_binary\"] = pd.to_numeric(df[\"Diabetes_binary\"], downcast=\"unsigned\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_and_optimize_dtypes(CSV_PATH, use_sample=False)\n",
    "print(\"Loaded dataframe shape:\", df.shape)\n",
    "print(\"\\nDtype summary (sample):\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4fea7b-0156-49d1-acb7-188c7af26c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution:\n",
      "Diabetes_binary\n",
      "0    218334\n",
      "1     35346\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target percentage positive:\n",
      "13.933301797540206\n",
      "\n",
      "Missing values per column (top 20):\n",
      "Diabetes_binary         0\n",
      "HighBP                  0\n",
      "Education               0\n",
      "Age                     0\n",
      "Sex                     0\n",
      "DiffWalk                0\n",
      "PhysHlth                0\n",
      "MentHlth                0\n",
      "GenHlth                 0\n",
      "NoDocbcCost             0\n",
      "AnyHealthcare           0\n",
      "HvyAlcoholConsump       0\n",
      "Veggies                 0\n",
      "Fruits                  0\n",
      "PhysActivity            0\n",
      "HeartDiseaseorAttack    0\n",
      "Stroke                  0\n",
      "Smoker                  0\n",
      "BMI                     0\n",
      "CholCheck               0\n",
      "dtype: int64\n",
      "\n",
      "Numeric summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>253680.0</td>\n",
       "      <td>28.382364</td>\n",
       "      <td>6.607666</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MentHlth</th>\n",
       "      <td>253680.0</td>\n",
       "      <td>3.184772</td>\n",
       "      <td>7.412847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PhysHlth</th>\n",
       "      <td>253680.0</td>\n",
       "      <td>4.242081</td>\n",
       "      <td>8.717951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GenHlth</th>\n",
       "      <td>253680.0</td>\n",
       "      <td>2.511392</td>\n",
       "      <td>1.068477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>253680.0</td>\n",
       "      <td>8.032119</td>\n",
       "      <td>3.054220</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       mean       std   min   25%   50%   75%   max\n",
       "BMI       253680.0  28.382364  6.607666  12.0  24.0  27.0  31.0  98.0\n",
       "MentHlth  253680.0   3.184772  7.412847   0.0   0.0   0.0   2.0  30.0\n",
       "PhysHlth  253680.0   4.242081  8.717951   0.0   0.0   0.0   3.0  30.0\n",
       "GenHlth   253680.0   2.511392  1.068477   1.0   2.0   2.0   3.0   5.0\n",
       "Age       253680.0   8.032119  3.054220   1.0   6.0   8.0  10.0  13.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 2: Quick EDA\n",
    "target_col = \"Diabetes_binary\"\n",
    "assert target_col in df.columns, f\"Target '{target_col}' not found.\"\n",
    "\n",
    "print(\"Target distribution:\")\n",
    "print(df[target_col].value_counts(dropna=False))\n",
    "print(\"\\nTarget percentage positive:\")\n",
    "print(df[target_col].mean() * 100)\n",
    "\n",
    "print(\"\\nMissing values per column (top 20):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "# A few summary stats for numeric columns\n",
    "num_cols = [\"BMI\", \"MentHlth\", \"PhysHlth\", \"GenHlth\", \"Age\"]\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "if num_cols:\n",
    "    print(\"\\nNumeric summary:\")\n",
    "    display(df[num_cols].describe().T)\n",
    "else:\n",
    "    print(\"\\nNo numeric columns from the expected list were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b68d24-427c-412d-9193-2e2d7c20a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using selected features only.\n",
      "Feature counts: 6\n",
      "Numeric columns: ['BMI', 'GenHlth', 'Age']\n",
      "Binary columns: ['HighBP', 'DiffWalk', 'HighChol']\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Prepare X and y using selected subset of columns\n",
    "target_col = \"Diabetes_binary\"\n",
    "selected_features = [\"GenHlth\", \"HighBP\", \"DiffWalk\", \"BMI\", \"HighChol\", \"Age\"]\n",
    "\n",
    "# Ensure all exist\n",
    "missing = [c for c in selected_features if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "X = df[selected_features].copy()\n",
    "y = df[target_col].astype(int)\n",
    "\n",
    "# Identify numeric vs binary\n",
    "# 'BMI' and 'Age' and 'GenHlth' are treated as numeric\n",
    "numeric_cols = [\"BMI\", \"GenHlth\", \"Age\"]\n",
    "binary_cols = [c for c in X.columns if c not in numeric_cols]\n",
    "\n",
    "print(\"Using selected features only.\")\n",
    "print(\"Feature counts:\", X.shape[1])\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Binary columns:\", binary_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5782019-e44b-446f-b5cf-ccb9a361a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes -> Train: (171234, 6) Val: (19026, 6) Test: (63420, 6)\n",
      "Train positive rate: 0.13932980599647266 Test positive rate: 0.13934090192368337\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Splits (75% train, 25% test; then val from train)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# Now create a validation set from X_temp (10% of training)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.10, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Sizes -> Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "print(\"Train positive rate:\", y_train.mean(), \"Test positive rate:\", y_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7505ad36-fa16-40ee-99cd-834e915b58d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor ready. Feature count after transform: 6\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Build preprocessor and fit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def build_preprocessor(numeric_cols, binary_cols):\n",
    "    numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"bin\", \"passthrough\", binary_cols)\n",
    "    ], remainder=\"drop\")\n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = build_preprocessor(numeric_cols, binary_cols)\n",
    "preprocessor.fit(X_train)  # fit only on train\n",
    "\n",
    "# Optionally sanity check\n",
    "_ = preprocessor.transform(X_train.iloc[:3])\n",
    "print(\"Preprocessor ready. Feature count after transform:\", preprocessor.transform(X_train[:1]).shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39fd1755-f62e-4379-a159-32e3ba7331ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Resampling libs + helpers\n",
    "!pip install -q imbalanced-learn xgboost\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Four class imbalance strategies\n",
    "samplers = {\n",
    "    \"SMOTE\": SMOTE(random_state=RANDOM_STATE),\n",
    "    \"ADASYN\": ADASYN(random_state=RANDOM_STATE),\n",
    "    \"SMOTE+Tomek\": SMOTETomek(random_state=RANDOM_STATE),\n",
    "    \"SMOTE+ENN\": SMOTEENN(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# We'll also include a \"None\" option to compare against no-resampling baseline\n",
    "samplers_with_none = {\"None\": None, **samplers}\n",
    "\n",
    "def pick_best_threshold(y_true_val, proba_val):\n",
    "    \"\"\"Pick threshold maximizing F1 on validation set (uses probabilities of positive class).\"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true_val, proba_val)\n",
    "    # precision_recall_curve returns thresholds of length n-1\n",
    "    f1s = (2 * precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-12)\n",
    "    best_idx = np.argmax(f1s)\n",
    "    best_thr = thresholds[best_idx]\n",
    "    return float(best_thr), float(f1s[best_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b9b7c21-9d33-4476-a7da-ae2e63b88c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7: Model builders\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "def build_model(name: str):\n",
    "    if name == \"LogisticRegression\":\n",
    "        return LogisticRegression(\n",
    "            solver=\"saga\", penalty=\"l2\", max_iter=2000,\n",
    "            class_weight=\"balanced\", random_state=RANDOM_STATE, n_jobs=1\n",
    "        )\n",
    "    if name == \"DecisionTree\":\n",
    "        return DecisionTreeClassifier(\n",
    "            max_depth=None, min_samples_split=10, min_samples_leaf=5,\n",
    "            class_weight=\"balanced\", random_state=RANDOM_STATE\n",
    "        )\n",
    "    if name == \"RandomForest\":\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_split=10, min_samples_leaf=5,\n",
    "            class_weight=\"balanced_subsample\", n_jobs=-1, random_state=RANDOM_STATE\n",
    "        )\n",
    "    if name == \"KNN\":\n",
    "        # KNN is sensitive to scaling; preprocessor handles it\n",
    "        return KNeighborsClassifier(n_neighbors=31, weights=\"distance\", metric=\"minkowski\")\n",
    "    if name == \"XGBoost\":\n",
    "        # use scale_pos_weight to address imbalance\n",
    "        pos = np.sum(y_train == 1); neg = np.sum(y_train == 0)\n",
    "        spw = max(1.0, neg / max(1.0, pos))\n",
    "        return XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            n_estimators=800, learning_rate=0.05,\n",
    "            max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "            reg_alpha=0.1, reg_lambda=0.5,\n",
    "            tree_method=\"hist\",\n",
    "            eval_metric=\"auc\",\n",
    "            random_state=RANDOM_STATE,\n",
    "            scale_pos_weight=spw,\n",
    "            n_jobs=1\n",
    "        )\n",
    "    if name == \"LightGBM\":\n",
    "        pos = np.sum(y_train == 1); neg = np.sum(y_train == 0)\n",
    "        spw = max(1.0, neg / max(1.0, pos))\n",
    "        return lgb.LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            boosting_type=\"gbdt\",\n",
    "            n_estimators=1200,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            max_depth=7,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1,\n",
    "            scale_pos_weight=spw\n",
    "        )\n",
    "    raise ValueError(f\"Unknown model name: {name}\")\n",
    "\n",
    "model_names = [\n",
    "    \"LogisticRegression\",\n",
    "    \"DecisionTree\",\n",
    "    \"RandomForest\",\n",
    "    \"KNN\",\n",
    "    \"XGBoost\",\n",
    "    \"LightGBM\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7711df2-decd-4648-a341-66e5445a8e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Resampling: None ===\n",
      "[LogisticRegression | None] -> {'model': 'LogisticRegression', 'sampler': 'None', 'thr_val_best_f1': 0.6398, 'val_best_f1': 0.4603, 'test_accuracy': 0.8025, 'test_precision': 0.3681, 'test_recall': 0.5822, 'test_f1': 0.4511, 'test_roc_auc': 0.8152, 'test_pr_auc': 0.3867}\n",
      "[DecisionTree | None] -> {'model': 'DecisionTree', 'sampler': 'None', 'thr_val_best_f1': 0.6383, 'val_best_f1': 0.4407, 'test_accuracy': 0.7865, 'test_precision': 0.3427, 'test_recall': 0.5795, 'test_f1': 0.4307, 'test_roc_auc': 0.7819, 'test_pr_auc': 0.3616}\n",
      "[RandomForest | None] -> {'model': 'RandomForest', 'sampler': 'None', 'thr_val_best_f1': 0.6295, 'val_best_f1': 0.446, 'test_accuracy': 0.7991, 'test_precision': 0.3591, 'test_recall': 0.5632, 'test_f1': 0.4386, 'test_roc_auc': 0.7982, 'test_pr_auc': 0.3832}\n",
      "[KNN | None] -> {'model': 'KNN', 'sampler': 'None', 'thr_val_best_f1': 0.2291, 'val_best_f1': 0.4352, 'test_accuracy': 0.7908, 'test_precision': 0.3466, 'test_recall': 0.566, 'test_f1': 0.4299, 'test_roc_auc': 0.7664, 'test_pr_auc': 0.3367}\n",
      "[XGBoost | None] -> {'model': 'XGBoost', 'sampler': 'None', 'thr_val_best_f1': 0.6523, 'val_best_f1': 0.4663, 'test_accuracy': 0.8033, 'test_precision': 0.3715, 'test_recall': 0.5952, 'test_f1': 0.4575, 'test_roc_auc': 0.818, 'test_pr_auc': 0.4078}\n",
      "[LightGBM] [Info] Number of positive: 23858, number of negative: 147376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101\n",
      "[LightGBM] [Info] Number of data points in the train set: 171234, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.139330 -> initscore=-1.820868\n",
      "[LightGBM] [Info] Start training from score -1.820868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM | None] -> {'model': 'LightGBM', 'sampler': 'None', 'thr_val_best_f1': 0.6584, 'val_best_f1': 0.4651, 'test_accuracy': 0.806, 'test_precision': 0.3751, 'test_recall': 0.5889, 'test_f1': 0.4583, 'test_roc_auc': 0.818, 'test_pr_auc': 0.407}\n",
      "\n",
      "=== Resampling: SMOTE ===\n",
      "[LogisticRegression | SMOTE] -> {'model': 'LogisticRegression', 'sampler': 'SMOTE', 'thr_val_best_f1': 0.6247, 'val_best_f1': 0.4607, 'test_accuracy': 0.7956, 'test_precision': 0.3608, 'test_recall': 0.6048, 'test_f1': 0.452, 'test_roc_auc': 0.8151, 'test_pr_auc': 0.3863}\n",
      "[DecisionTree | SMOTE] -> {'model': 'DecisionTree', 'sampler': 'SMOTE', 'thr_val_best_f1': 0.5949, 'val_best_f1': 0.4248, 'test_accuracy': 0.7902, 'test_precision': 0.3425, 'test_recall': 0.5502, 'test_f1': 0.4222, 'test_roc_auc': 0.7671, 'test_pr_auc': 0.3503}\n",
      "[RandomForest | SMOTE] -> {'model': 'RandomForest', 'sampler': 'SMOTE', 'thr_val_best_f1': 0.6242, 'val_best_f1': 0.4427, 'test_accuracy': 0.8006, 'test_precision': 0.359, 'test_recall': 0.5485, 'test_f1': 0.434, 'test_roc_auc': 0.7998, 'test_pr_auc': 0.3836}\n",
      "[KNN | SMOTE] -> {'model': 'KNN', 'sampler': 'SMOTE', 'thr_val_best_f1': 0.2353, 'val_best_f1': 0.4299, 'test_accuracy': 0.7672, 'test_precision': 0.3241, 'test_recall': 0.6177, 'test_f1': 0.4252, 'test_roc_auc': 0.7612, 'test_pr_auc': 0.3238}\n",
      "[XGBoost | SMOTE] -> {'model': 'XGBoost', 'sampler': 'SMOTE', 'thr_val_best_f1': 0.8904, 'val_best_f1': 0.4575, 'test_accuracy': 0.7827, 'test_precision': 0.3485, 'test_recall': 0.6441, 'test_f1': 0.4523, 'test_roc_auc': 0.8128, 'test_pr_auc': 0.4046}\n",
      "[LightGBM] [Info] Number of positive: 147376, number of negative: 147376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 736\n",
      "[LightGBM] [Info] Number of data points in the train set: 294752, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM | SMOTE] -> {'model': 'LightGBM', 'sampler': 'SMOTE', 'thr_val_best_f1': 0.8988, 'val_best_f1': 0.4509, 'test_accuracy': 0.8012, 'test_precision': 0.3638, 'test_recall': 0.5694, 'test_f1': 0.444, 'test_roc_auc': 0.8052, 'test_pr_auc': 0.3956}\n",
      "\n",
      "=== Resampling: ADASYN ===\n",
      "[LogisticRegression | ADASYN] -> {'model': 'LogisticRegression', 'sampler': 'ADASYN', 'thr_val_best_f1': 0.6276, 'val_best_f1': 0.4602, 'test_accuracy': 0.7896, 'test_precision': 0.3545, 'test_recall': 0.6216, 'test_f1': 0.4515, 'test_roc_auc': 0.8146, 'test_pr_auc': 0.3853}\n",
      "[DecisionTree | ADASYN] -> {'model': 'DecisionTree', 'sampler': 'ADASYN', 'thr_val_best_f1': 0.5949, 'val_best_f1': 0.4243, 'test_accuracy': 0.7757, 'test_precision': 0.3233, 'test_recall': 0.5575, 'test_f1': 0.4093, 'test_roc_auc': 0.7592, 'test_pr_auc': 0.3366}\n",
      "[RandomForest | ADASYN] -> {'model': 'RandomForest', 'sampler': 'ADASYN', 'thr_val_best_f1': 0.6111, 'val_best_f1': 0.4366, 'test_accuracy': 0.7829, 'test_precision': 0.3368, 'test_recall': 0.5754, 'test_f1': 0.4249, 'test_roc_auc': 0.7931, 'test_pr_auc': 0.369}\n",
      "[KNN | ADASYN] -> {'model': 'KNN', 'sampler': 'ADASYN', 'thr_val_best_f1': 0.2467, 'val_best_f1': 0.428, 'test_accuracy': 0.7656, 'test_precision': 0.3227, 'test_recall': 0.6207, 'test_f1': 0.4246, 'test_roc_auc': 0.7604, 'test_pr_auc': 0.3214}\n",
      "[XGBoost | ADASYN] -> {'model': 'XGBoost', 'sampler': 'ADASYN', 'thr_val_best_f1': 0.9054, 'val_best_f1': 0.4559, 'test_accuracy': 0.7863, 'test_precision': 0.3506, 'test_recall': 0.626, 'test_f1': 0.4494, 'test_roc_auc': 0.8105, 'test_pr_auc': 0.4011}\n",
      "[LightGBM] [Info] Number of positive: 148986, number of negative: 147376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 864\n",
      "[LightGBM] [Info] Number of data points in the train set: 296362, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502716 -> initscore=0.010865\n",
      "[LightGBM] [Info] Start training from score 0.010865\n",
      "[LightGBM | ADASYN] -> {'model': 'LightGBM', 'sampler': 'ADASYN', 'thr_val_best_f1': 0.892, 'val_best_f1': 0.444, 'test_accuracy': 0.7793, 'test_precision': 0.3389, 'test_recall': 0.6142, 'test_f1': 0.4368, 'test_roc_auc': 0.7998, 'test_pr_auc': 0.3881}\n",
      "\n",
      "=== Resampling: SMOTE+Tomek ===\n",
      "[LogisticRegression | SMOTE+Tomek] -> {'model': 'LogisticRegression', 'sampler': 'SMOTE+Tomek', 'thr_val_best_f1': 0.6286, 'val_best_f1': 0.4608, 'test_accuracy': 0.7968, 'test_precision': 0.3619, 'test_recall': 0.6008, 'test_f1': 0.4517, 'test_roc_auc': 0.8152, 'test_pr_auc': 0.3864}\n",
      "[DecisionTree | SMOTE+Tomek] -> {'model': 'DecisionTree', 'sampler': 'SMOTE+Tomek', 'thr_val_best_f1': 0.6212, 'val_best_f1': 0.4276, 'test_accuracy': 0.8006, 'test_precision': 0.3527, 'test_recall': 0.5159, 'test_f1': 0.419, 'test_roc_auc': 0.7669, 'test_pr_auc': 0.349}\n",
      "[RandomForest | SMOTE+Tomek] -> {'model': 'RandomForest', 'sampler': 'SMOTE+Tomek', 'thr_val_best_f1': 0.5916, 'val_best_f1': 0.4432, 'test_accuracy': 0.7873, 'test_precision': 0.3454, 'test_recall': 0.5883, 'test_f1': 0.4352, 'test_roc_auc': 0.7999, 'test_pr_auc': 0.3836}\n",
      "[KNN | SMOTE+Tomek] -> {'model': 'KNN', 'sampler': 'SMOTE+Tomek', 'thr_val_best_f1': 0.2353, 'val_best_f1': 0.4301, 'test_accuracy': 0.7668, 'test_precision': 0.3242, 'test_recall': 0.6209, 'test_f1': 0.426, 'test_roc_auc': 0.763, 'test_pr_auc': 0.3246}\n",
      "[XGBoost | SMOTE+Tomek] -> {'model': 'XGBoost', 'sampler': 'SMOTE+Tomek', 'thr_val_best_f1': 0.9007, 'val_best_f1': 0.4607, 'test_accuracy': 0.7912, 'test_precision': 0.3567, 'test_recall': 0.6206, 'test_f1': 0.453, 'test_roc_auc': 0.8131, 'test_pr_auc': 0.4061}\n",
      "[LightGBM] [Info] Number of positive: 147132, number of negative: 147132\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 733\n",
      "[LightGBM] [Info] Number of data points in the train set: 294264, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM | SMOTE+Tomek] -> {'model': 'LightGBM', 'sampler': 'SMOTE+Tomek', 'thr_val_best_f1': 0.9049, 'val_best_f1': 0.4512, 'test_accuracy': 0.8062, 'test_precision': 0.3689, 'test_recall': 0.55, 'test_f1': 0.4416, 'test_roc_auc': 0.8055, 'test_pr_auc': 0.3967}\n",
      "\n",
      "=== Resampling: SMOTE+ENN ===\n",
      "[LogisticRegression | SMOTE+ENN] -> {'model': 'LogisticRegression', 'sampler': 'SMOTE+ENN', 'thr_val_best_f1': 0.6095, 'val_best_f1': 0.453, 'test_accuracy': 0.7788, 'test_precision': 0.3399, 'test_recall': 0.6235, 'test_f1': 0.4399, 'test_roc_auc': 0.8091, 'test_pr_auc': 0.3736}\n",
      "[DecisionTree | SMOTE+ENN] -> {'model': 'DecisionTree', 'sampler': 'SMOTE+ENN', 'thr_val_best_f1': 0.2149, 'val_best_f1': 0.3391, 'test_accuracy': 0.8158, 'test_precision': 0.3365, 'test_recall': 0.331, 'test_f1': 0.3337, 'test_roc_auc': 0.6127, 'test_pr_auc': 0.227}\n",
      "[RandomForest | SMOTE+ENN] -> {'model': 'RandomForest', 'sampler': 'SMOTE+ENN', 'thr_val_best_f1': 0.245, 'val_best_f1': 0.3864, 'test_accuracy': 0.7491, 'test_precision': 0.2873, 'test_recall': 0.5409, 'test_f1': 0.3753, 'test_roc_auc': 0.7444, 'test_pr_auc': 0.334}\n",
      "[KNN | SMOTE+ENN] -> {'model': 'KNN', 'sampler': 'SMOTE+ENN', 'thr_val_best_f1': 0.2079, 'val_best_f1': 0.3465, 'test_accuracy': 0.7706, 'test_precision': 0.2847, 'test_recall': 0.4271, 'test_f1': 0.3416, 'test_roc_auc': 0.645, 'test_pr_auc': 0.253}\n",
      "[XGBoost | SMOTE+ENN] -> {'model': 'XGBoost', 'sampler': 'SMOTE+ENN', 'thr_val_best_f1': 0.3424, 'val_best_f1': 0.4207, 'test_accuracy': 0.7568, 'test_precision': 0.3101, 'test_recall': 0.6081, 'test_f1': 0.4107, 'test_roc_auc': 0.7543, 'test_pr_auc': 0.3627}\n",
      "[LightGBM] [Info] Number of positive: 33047, number of negative: 108541\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 141588, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.233403 -> initscore=-1.189197\n",
      "[LightGBM] [Info] Start training from score -1.189197\n",
      "[LightGBM | SMOTE+ENN] -> {'model': 'LightGBM', 'sampler': 'SMOTE+ENN', 'thr_val_best_f1': 0.0681, 'val_best_f1': 0.4218, 'test_accuracy': 0.7906, 'test_precision': 0.3384, 'test_recall': 0.527, 'test_f1': 0.4122, 'test_roc_auc': 0.7601, 'test_pr_auc': 0.3602}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sampler</th>\n",
       "      <th>thr_val_best_f1</th>\n",
       "      <th>val_best_f1</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>None</td>\n",
       "      <td>0.638326</td>\n",
       "      <td>0.440692</td>\n",
       "      <td>0.786534</td>\n",
       "      <td>0.342702</td>\n",
       "      <td>0.579495</td>\n",
       "      <td>0.430698</td>\n",
       "      <td>0.781890</td>\n",
       "      <td>0.361603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.424842</td>\n",
       "      <td>0.790161</td>\n",
       "      <td>0.342515</td>\n",
       "      <td>0.550187</td>\n",
       "      <td>0.422195</td>\n",
       "      <td>0.767107</td>\n",
       "      <td>0.350328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>SMOTE+Tomek</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.427643</td>\n",
       "      <td>0.800615</td>\n",
       "      <td>0.352700</td>\n",
       "      <td>0.515899</td>\n",
       "      <td>0.418968</td>\n",
       "      <td>0.766906</td>\n",
       "      <td>0.348998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>ADASYN</td>\n",
       "      <td>0.594945</td>\n",
       "      <td>0.424309</td>\n",
       "      <td>0.775733</td>\n",
       "      <td>0.323294</td>\n",
       "      <td>0.557542</td>\n",
       "      <td>0.409270</td>\n",
       "      <td>0.759167</td>\n",
       "      <td>0.336586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>SMOTE+ENN</td>\n",
       "      <td>0.214888</td>\n",
       "      <td>0.339079</td>\n",
       "      <td>0.815847</td>\n",
       "      <td>0.336516</td>\n",
       "      <td>0.330995</td>\n",
       "      <td>0.333733</td>\n",
       "      <td>0.612666</td>\n",
       "      <td>0.227032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>None</td>\n",
       "      <td>0.229143</td>\n",
       "      <td>0.435197</td>\n",
       "      <td>0.790823</td>\n",
       "      <td>0.346567</td>\n",
       "      <td>0.566029</td>\n",
       "      <td>0.429910</td>\n",
       "      <td>0.766441</td>\n",
       "      <td>0.336730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>SMOTE+Tomek</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.430143</td>\n",
       "      <td>0.766824</td>\n",
       "      <td>0.324195</td>\n",
       "      <td>0.620912</td>\n",
       "      <td>0.425976</td>\n",
       "      <td>0.762977</td>\n",
       "      <td>0.324606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.429856</td>\n",
       "      <td>0.767234</td>\n",
       "      <td>0.324111</td>\n",
       "      <td>0.617744</td>\n",
       "      <td>0.425156</td>\n",
       "      <td>0.761249</td>\n",
       "      <td>0.323787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>ADASYN</td>\n",
       "      <td>0.246732</td>\n",
       "      <td>0.428007</td>\n",
       "      <td>0.765626</td>\n",
       "      <td>0.322704</td>\n",
       "      <td>0.620686</td>\n",
       "      <td>0.424634</td>\n",
       "      <td>0.760416</td>\n",
       "      <td>0.321436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN</td>\n",
       "      <td>SMOTE+ENN</td>\n",
       "      <td>0.207865</td>\n",
       "      <td>0.346487</td>\n",
       "      <td>0.770640</td>\n",
       "      <td>0.284680</td>\n",
       "      <td>0.427068</td>\n",
       "      <td>0.341631</td>\n",
       "      <td>0.644953</td>\n",
       "      <td>0.253016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>None</td>\n",
       "      <td>0.658374</td>\n",
       "      <td>0.465076</td>\n",
       "      <td>0.805992</td>\n",
       "      <td>0.375063</td>\n",
       "      <td>0.588888</td>\n",
       "      <td>0.458260</td>\n",
       "      <td>0.817959</td>\n",
       "      <td>0.406972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.898753</td>\n",
       "      <td>0.450874</td>\n",
       "      <td>0.801246</td>\n",
       "      <td>0.363794</td>\n",
       "      <td>0.569424</td>\n",
       "      <td>0.443954</td>\n",
       "      <td>0.805154</td>\n",
       "      <td>0.395626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>SMOTE+Tomek</td>\n",
       "      <td>0.904909</td>\n",
       "      <td>0.451173</td>\n",
       "      <td>0.806213</td>\n",
       "      <td>0.368936</td>\n",
       "      <td>0.549960</td>\n",
       "      <td>0.441617</td>\n",
       "      <td>0.805477</td>\n",
       "      <td>0.396726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>ADASYN</td>\n",
       "      <td>0.892030</td>\n",
       "      <td>0.443966</td>\n",
       "      <td>0.779313</td>\n",
       "      <td>0.338932</td>\n",
       "      <td>0.614236</td>\n",
       "      <td>0.436826</td>\n",
       "      <td>0.799770</td>\n",
       "      <td>0.388064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>SMOTE+ENN</td>\n",
       "      <td>0.068114</td>\n",
       "      <td>0.421761</td>\n",
       "      <td>0.790555</td>\n",
       "      <td>0.338445</td>\n",
       "      <td>0.526989</td>\n",
       "      <td>0.412179</td>\n",
       "      <td>0.760137</td>\n",
       "      <td>0.360192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.460693</td>\n",
       "      <td>0.795632</td>\n",
       "      <td>0.360807</td>\n",
       "      <td>0.604843</td>\n",
       "      <td>0.451989</td>\n",
       "      <td>0.815135</td>\n",
       "      <td>0.386332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>SMOTE+Tomek</td>\n",
       "      <td>0.628585</td>\n",
       "      <td>0.460819</td>\n",
       "      <td>0.796783</td>\n",
       "      <td>0.361920</td>\n",
       "      <td>0.600769</td>\n",
       "      <td>0.451714</td>\n",
       "      <td>0.815207</td>\n",
       "      <td>0.386365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>ADASYN</td>\n",
       "      <td>0.627581</td>\n",
       "      <td>0.460167</td>\n",
       "      <td>0.789593</td>\n",
       "      <td>0.354547</td>\n",
       "      <td>0.621591</td>\n",
       "      <td>0.451541</td>\n",
       "      <td>0.814586</td>\n",
       "      <td>0.385259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>None</td>\n",
       "      <td>0.639824</td>\n",
       "      <td>0.460345</td>\n",
       "      <td>0.802539</td>\n",
       "      <td>0.368131</td>\n",
       "      <td>0.582211</td>\n",
       "      <td>0.451059</td>\n",
       "      <td>0.815155</td>\n",
       "      <td>0.386681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>SMOTE+ENN</td>\n",
       "      <td>0.609504</td>\n",
       "      <td>0.452980</td>\n",
       "      <td>0.778776</td>\n",
       "      <td>0.339851</td>\n",
       "      <td>0.623515</td>\n",
       "      <td>0.439920</td>\n",
       "      <td>0.809067</td>\n",
       "      <td>0.373606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model      sampler  thr_val_best_f1  val_best_f1  \\\n",
       "0         DecisionTree         None         0.638326     0.440692   \n",
       "1         DecisionTree        SMOTE         0.594937     0.424842   \n",
       "2         DecisionTree  SMOTE+Tomek         0.621212     0.427643   \n",
       "3         DecisionTree       ADASYN         0.594945     0.424309   \n",
       "4         DecisionTree    SMOTE+ENN         0.214888     0.339079   \n",
       "5                  KNN         None         0.229143     0.435197   \n",
       "6                  KNN  SMOTE+Tomek         0.235294     0.430143   \n",
       "7                  KNN        SMOTE         0.235294     0.429856   \n",
       "8                  KNN       ADASYN         0.246732     0.428007   \n",
       "9                  KNN    SMOTE+ENN         0.207865     0.346487   \n",
       "10            LightGBM         None         0.658374     0.465076   \n",
       "11            LightGBM        SMOTE         0.898753     0.450874   \n",
       "12            LightGBM  SMOTE+Tomek         0.904909     0.451173   \n",
       "13            LightGBM       ADASYN         0.892030     0.443966   \n",
       "14            LightGBM    SMOTE+ENN         0.068114     0.421761   \n",
       "15  LogisticRegression        SMOTE         0.624687     0.460693   \n",
       "16  LogisticRegression  SMOTE+Tomek         0.628585     0.460819   \n",
       "17  LogisticRegression       ADASYN         0.627581     0.460167   \n",
       "18  LogisticRegression         None         0.639824     0.460345   \n",
       "19  LogisticRegression    SMOTE+ENN         0.609504     0.452980   \n",
       "\n",
       "    test_accuracy  test_precision  test_recall   test_f1  test_roc_auc  \\\n",
       "0        0.786534        0.342702     0.579495  0.430698      0.781890   \n",
       "1        0.790161        0.342515     0.550187  0.422195      0.767107   \n",
       "2        0.800615        0.352700     0.515899  0.418968      0.766906   \n",
       "3        0.775733        0.323294     0.557542  0.409270      0.759167   \n",
       "4        0.815847        0.336516     0.330995  0.333733      0.612666   \n",
       "5        0.790823        0.346567     0.566029  0.429910      0.766441   \n",
       "6        0.766824        0.324195     0.620912  0.425976      0.762977   \n",
       "7        0.767234        0.324111     0.617744  0.425156      0.761249   \n",
       "8        0.765626        0.322704     0.620686  0.424634      0.760416   \n",
       "9        0.770640        0.284680     0.427068  0.341631      0.644953   \n",
       "10       0.805992        0.375063     0.588888  0.458260      0.817959   \n",
       "11       0.801246        0.363794     0.569424  0.443954      0.805154   \n",
       "12       0.806213        0.368936     0.549960  0.441617      0.805477   \n",
       "13       0.779313        0.338932     0.614236  0.436826      0.799770   \n",
       "14       0.790555        0.338445     0.526989  0.412179      0.760137   \n",
       "15       0.795632        0.360807     0.604843  0.451989      0.815135   \n",
       "16       0.796783        0.361920     0.600769  0.451714      0.815207   \n",
       "17       0.789593        0.354547     0.621591  0.451541      0.814586   \n",
       "18       0.802539        0.368131     0.582211  0.451059      0.815155   \n",
       "19       0.778776        0.339851     0.623515  0.439920      0.809067   \n",
       "\n",
       "    test_pr_auc  \n",
       "0      0.361603  \n",
       "1      0.350328  \n",
       "2      0.348998  \n",
       "3      0.336586  \n",
       "4      0.227032  \n",
       "5      0.336730  \n",
       "6      0.324606  \n",
       "7      0.323787  \n",
       "8      0.321436  \n",
       "9      0.253016  \n",
       "10     0.406972  \n",
       "11     0.395626  \n",
       "12     0.396726  \n",
       "13     0.388064  \n",
       "14     0.360192  \n",
       "15     0.386332  \n",
       "16     0.386365  \n",
       "17     0.385259  \n",
       "18     0.386681  \n",
       "19     0.373606  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: model_output/imbalance_model_results 5 features.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 8: Train/eval loop with resampling and threshold tuning\n",
    "def evaluate_pipeline(model_name, sampler_name, sampler, verbose=False):\n",
    "    \"\"\"Build pipeline: Preprocess -> (Sampler?) -> Model, tune threshold on val, then test.\"\"\"\n",
    "    # Build steps\n",
    "    steps = [(\"preprocessor\", preprocessor)]\n",
    "    if sampler is not None:\n",
    "        steps.append((\"sampler\", sampler))\n",
    "    steps.append((\"clf\", build_model(model_name)))\n",
    "    \n",
    "    pipe = ImbPipeline(steps=steps)\n",
    "\n",
    "    # Fit on train only\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Validation probabilities for threshold selection\n",
    "    proba_val = pipe.predict_proba(X_val)[:, 1]\n",
    "    thr, best_val_f1 = pick_best_threshold(y_val, proba_val)\n",
    "\n",
    "    # Evaluate on test at tuned threshold\n",
    "    proba_test = pipe.predict_proba(X_test)[:, 1]\n",
    "    preds_test = (proba_test >= thr).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"sampler\": sampler_name,\n",
    "        \"thr_val_best_f1\": thr,\n",
    "        \"val_best_f1\": best_val_f1,\n",
    "        \"test_accuracy\": accuracy_score(y_test, preds_test),\n",
    "        \"test_precision\": precision_score(y_test, preds_test, zero_division=0),\n",
    "        \"test_recall\": recall_score(y_test, preds_test, zero_division=0),\n",
    "        \"test_f1\": f1_score(y_test, preds_test, zero_division=0),\n",
    "        \"test_roc_auc\": roc_auc_score(y_test, proba_test),\n",
    "        \"test_pr_auc\": average_precision_score(y_test, proba_test)\n",
    "    }\n",
    "    if verbose:\n",
    "        print(f\"[{model_name} | {sampler_name}] ->\", {k: round(v,4) if isinstance(v, float) else v for k,v in metrics.items()})\n",
    "    return metrics, pipe\n",
    "\n",
    "all_results = []\n",
    "best_pipelines = {}  # (model_name, sampler_name) -> fitted pipeline\n",
    "\n",
    "for sampler_name, sampler in samplers_with_none.items():\n",
    "    print(f\"\\n=== Resampling: {sampler_name} ===\")\n",
    "    for model_name in model_names:\n",
    "        res, pipe = evaluate_pipeline(model_name, sampler_name, sampler, verbose=True)\n",
    "        all_results.append(res)\n",
    "        best_pipelines[(model_name, sampler_name)] = pipe\n",
    "\n",
    "results_df = pd.DataFrame(all_results).sort_values([\"model\", \"test_f1\"], ascending=[True, False]).reset_index(drop=True)\n",
    "display(results_df.head(20))\n",
    "\n",
    "results_csv = Path(OUTPUT_DIR) / \"imbalance_model_results 5 features.csv\"\n",
    "results_df.to_csv(results_csv, index=False)\n",
    "print(\"Saved results to:\", results_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf1693fb-7560-4085-adf3-a97f6c2c0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Param combos: 10\n",
      "\n",
      "=== Fit 1/10: {'scale_pos_weight': 4.0, 'learning_rate': 0.02, 'num_leaves': 31, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'n_estimators': 5000, 'boosting_type': 'gbdt', 'random_state': 42, 'n_jobs': 1} ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_proc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m clf \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp)\n\u001b[1;32m     90\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     91\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m---> 92\u001b[0m     X_train_proc, y_train,\n\u001b[1;32m     93\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_val_proc, y_val)],\n\u001b[1;32m     94\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     95\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[lgb\u001b[38;5;241m.\u001b[39mearly_stopping(stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Validation probabilities\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_proc' is not defined"
     ]
    }
   ],
   "source": [
    "# Part 8: LightGBM  imbalance fixes + threshold tuning (val-based)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score, roc_auc_score,\n",
    "    average_precision_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def compute_metrics(y_true, proba, threshold=0.5, prefix=\"\"):\n",
    "    preds = (proba >= threshold).astype(int)\n",
    "    out = {\n",
    "        f\"{prefix}threshold\": float(threshold),\n",
    "        f\"{prefix}accuracy\": accuracy_score(y_true, preds),\n",
    "        f\"{prefix}precision\": precision_score(y_true, preds, zero_division=0),\n",
    "        f\"{prefix}recall\": recall_score(y_true, preds, zero_division=0),\n",
    "        f\"{prefix}f1\": f1_score(y_true, preds, zero_division=0),\n",
    "        f\"{prefix}roc_auc\": roc_auc_score(y_true, proba),\n",
    "        f\"{prefix}pr_auc\": average_precision_score(y_true, proba),\n",
    "        f\"{prefix}positive_rate_pred\": float(preds.mean()),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def tune_threshold_for_f1(y_true, proba, grid=None):\n",
    "    if grid is None:\n",
    "        # try a dense grid in the range that usually helps for ~1020% positives\n",
    "        grid = np.linspace(0.05, 0.5, 46)  # 0.05, 0.06, ..., 0.50\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        f1 = f1_score(y_true, (proba >= t).astype(int), zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, float(t)\n",
    "    return best_t, best_f1\n",
    "\n",
    "def plot_curves(y_true, proba, name_prefix):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc_score(y_true, proba):.3f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"ROC  {name_prefix}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    roc_path = os.path.join(OUTPUT_DIR, f\"{name_prefix}_ROC.png\")\n",
    "    plt.savefig(roc_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # PR\n",
    "    prec, rec, _ = precision_recall_curve(y_true, proba)\n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec, label=f\"AP={average_precision_score(y_true, proba):.3f}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"Precision-Recall  {name_prefix}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    pr_path = os.path.join(OUTPUT_DIR, f\"{name_prefix}_PR.png\")\n",
    "    plt.savefig(pr_path, bbox_inches=\"tight\"); plt.close()\n",
    "    return {\"roc_plot\": roc_path, \"pr_plot\": pr_path}\n",
    "\n",
    "# ---------- small param sweep ----------\n",
    "param_grid = []\n",
    "for w in [4.0, 5.0, 6.0, 7.0, 8.0]:       # imbalance strength\n",
    "    for lr in [0.02, 0.03]:               # learn slower, train longer\n",
    "        param_grid.append({\n",
    "            \"scale_pos_weight\": w,\n",
    "            \"learning_rate\": lr,\n",
    "            \"num_leaves\": 31,\n",
    "            \"max_depth\": 7,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"reg_alpha\": 0.1,\n",
    "            \"reg_lambda\": 0.1,\n",
    "            \"n_estimators\": 5000,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"n_jobs\": 1\n",
    "        })\n",
    "\n",
    "val_results = []\n",
    "best_record = None\n",
    "\n",
    "print(f\"[INFO] Param combos: {len(param_grid)}\")\n",
    "for i, p in enumerate(param_grid, 1):\n",
    "    print(f\"\\n=== Fit {i}/{len(param_grid)}: {p} ===\")\n",
    "    clf = lgb.LGBMClassifier(**p)\n",
    "    start = time.time()\n",
    "    clf.fit(\n",
    "        X_train_proc, y_train,\n",
    "        eval_set=[(X_val_proc, y_val)],\n",
    "        eval_metric=[\"auc\", \"average_precision\"],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    # Validation probabilities\n",
    "    val_proba = clf.predict_proba(X_val_proc)[:, 1]\n",
    "\n",
    "    # Metrics at default 0.5\n",
    "    val_metrics_05 = compute_metrics(y_val, val_proba, threshold=0.5, prefix=\"val@0.5_\")\n",
    "\n",
    "    # Tune threshold for F1 on validation\n",
    "    best_t, best_f1 = tune_threshold_for_f1(y_val, val_proba)\n",
    "    val_metrics_best = compute_metrics(y_val, val_proba, threshold=best_t, prefix=\"val@best_\")\n",
    "\n",
    "    # Keep record\n",
    "    record = {\n",
    "        \"params\": p,\n",
    "        \"best_iteration\": int(getattr(clf, \"best_iteration_\", p[\"n_estimators\"]) or p[\"n_estimators\"]),\n",
    "        \"train_time_sec\": round(train_time, 3),\n",
    "        **val_metrics_05,\n",
    "        **val_metrics_best,\n",
    "        \"val_best_threshold\": best_t\n",
    "    }\n",
    "    val_results.append(record)\n",
    "\n",
    "    # Track best by F1 on validation (tie-break by PR-AUC)\n",
    "    if (best_record is None) or \\\n",
    "       (record[\"val@best_f1\"] > best_record[\"val@best_f1\"]) or \\\n",
    "       (np.isclose(record[\"val@best_f1\"], best_record[\"val@best_f1\"]) and\n",
    "        record[\"val@best_pr_auc\"] > best_record[\"val@best_pr_auc\"]):\n",
    "        best_record = record\n",
    "        best_model = clf\n",
    "\n",
    "print(\"\\n[SUMMARY] Validation sweep (top 5 by F1):\")\n",
    "df_val = pd.DataFrame(val_results).sort_values(\"val@best_f1\", ascending=False)\n",
    "display(df_val.head(5))\n",
    "\n",
    "# Save sweep table\n",
    "val_csv = os.path.join(OUTPUT_DIR, \"lgbm_val_sweep.csv\")\n",
    "df_val.to_csv(val_csv, index=False)\n",
    "print(\"Saved:\", val_csv)\n",
    "\n",
    "print(\"\\n[WINNER] Best validation config:\")\n",
    "pprint({k: best_record[k] for k in [\n",
    "    \"val@best_f1\",\"val@best_precision\",\"val@best_recall\",\"val@best_pr_auc\",\"val@best_roc_auc\",\"val_best_threshold\",\n",
    "    \"val@0.5_f1\",\"val@0.5_precision\",\"val@0.5_recall\",\"val@0.5_pr_auc\",\"best_iteration\",\"train_time_sec\"\n",
    "]})\n",
    "print(\"\\nBest params:\")\n",
    "pprint(best_record[\"params\"])\n",
    "\n",
    "# Plots on validation for the best model\n",
    "val_plots = plot_curves(y_val, best_model.predict_proba(X_val_proc)[:,1], \"LightGBM_val_best\")\n",
    "print(\"Validation plots saved:\", val_plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6d3c7-fa89-4bf7-8051-c2fd95c18576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 10: Inspect best per model by test F1 and show top importances for tree boosters\n",
    "best_rows = results_df.sort_values(\"test_f1\", ascending=False).groupby(\"model\", as_index=False).first()\n",
    "display(best_rows[[\"model\",\"sampler\",\"thr_val_best_f1\",\"test_f1\",\"test_precision\",\"test_recall\",\"test_pr_auc\",\"test_roc_auc\"]])\n",
    "\n",
    "def show_top_importances(model_name, sampler_name, top_k=15):\n",
    "    pipe = best_pipelines[(model_name, sampler_name)]\n",
    "    clf = pipe.named_steps[\"clf\"]\n",
    "    # Get feature names post-preprocessing\n",
    "    feat_names_num = [f\"num__{c}\" for c in numeric_cols]\n",
    "    feat_names_bin = binary_cols[:]  # passthrough keeps original names\n",
    "    feature_names = feat_names_num + feat_names_bin\n",
    "\n",
    "    importances = None\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        importances = clf.feature_importances_\n",
    "    elif hasattr(clf, \"coef_\"):\n",
    "        # Logistic regression (absolute coefficients)\n",
    "        coef = np.ravel(clf.coef_)\n",
    "        importances = np.abs(coef)\n",
    "    else:\n",
    "        print(f\"No importances available for {model_name}.\")\n",
    "        return\n",
    "\n",
    "    imp_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False).head(top_k)\n",
    "    display(imp_df)\n",
    "\n",
    "# Example: show importances for your best LightGBM variant if it appears in best_rows\n",
    "for _, row in best_rows.iterrows():\n",
    "    if row[\"model\"] in [\"LightGBM\",\"RandomForest\",\"XGBoost\",\"LogisticRegression\"]:\n",
    "        print(f\"\\nTop features for {row['model']} ({row['sampler']}):\")\n",
    "        show_top_importances(row[\"model\"], row[\"sampler\"], top_k=15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
